{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out different optimizers\n",
    "\n",
    "We begin with building a CNN architecture for image classification task on CIFAR10 dataset. In this part of the tutorial, we will understand how to use  different optimizers to train a CNN network.\n",
    "\n",
    "To make data loading simple, we would use the torchvision package created as part of PyTorch which has data loaders for standard datasets such as ImageNet, CIFAR10, MNIST.\n",
    "\n",
    "### CIFAR10 dataset\n",
    "![CIFAR10](images/cifar10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a Tensor library with GPU support\n",
    "import torch\n",
    "\n",
    "#Datasets, Transforms and Models specific to Computer Vision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#differentiation library that supports all differentiable Tensor operations in torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#a neural networks library integrated with autograd functionality\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#an optimization package with standard optimization methods such as SGD, RMSProp, LBFGS, Adam etc.\n",
    "import torch.optim as optim\n",
    "\n",
    "#Weight Initialization\n",
    "import torch.nn.init as weight_init\n",
    "\n",
    "# import nn module\n",
    "import torch.nn as nn\n",
    "\n",
    "#scientific computing library for Python\n",
    "import numpy as np\n",
    "\n",
    "#plotting and visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "#Display on the notebook\n",
    "%matplotlib inline \n",
    "plt.ion() #Turn interactive mode on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train data\n",
    "#Compose transforms (applies data transformation and augmentation) prior to feeding to training\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#inbuilt dataset class for reading CIFAR10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='../../data/lab1', train=True,\n",
    "                                        download=False, transform=transform)\n",
    "\n",
    "#dataloader for Batching, shuffling and loading data in parallel\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "#test data\n",
    "testset = torchvision.datasets.CIFAR10(root='../../data/lab1', train=False,\n",
    "                                       download=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Defining the model\n",
    "\n",
    "To create a network, we should first inherit the base class nn.Module. You just have to define the forward function, and the backward function (where gradients are computed) is automatically defined for you using autograd. You can use any of the Tensor operations in the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model (\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool_1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool_2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (fc1): Linear (400 -> 120)\n",
      "  (fc2): Linear (120 -> 84)\n",
      "  (fc3): Linear (84 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        #calling conv2d module for convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5,stride=1,padding=0,bias=True)\n",
    "        \n",
    "        #calling MaxPool2d module for max pooling with downsampling of 2\n",
    "        self.pool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.pool_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        #fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)   \n",
    "     \n",
    "    #defining the structure of the network\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Applying relu activation after each conv layer\n",
    "        x = self.pool_1(F.relu(self.conv1(x)))\n",
    "        x = self.pool_2(F.relu(self.conv2(x)))\n",
    "        \n",
    "        #reshaping to 1d for giving input to fully connected units\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Model()\n",
    "model = model.cuda()\n",
    "\n",
    "#Printing the network architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using different kind of optimizers\n",
    "\n",
    "For more details on each of the optimizers please refer the following: http://ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimization scheme can be 'sgd', 'RMSProp', 'Adam', 'Adadelta', 'Adagrad'\n",
    "optimization_scheme = \"Adagrad\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if optimization_scheme == 'sgd':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "elif optimization_scheme == 'RMSProp':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0)\n",
    "elif optimization_scheme == \"Adadelta\":\n",
    "     optimizer = optim.Adadelta(model.parameters(), lr=0.001, weight_decay=0)\n",
    "elif optimization_scheme == \"Adam\":\n",
    "     optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0)\n",
    "elif optimization_scheme == \"Adagrad\":\n",
    "     optimizer = optim.Adagrad(model.parameters(), lr=0.001, weight_decay=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with the defined optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss: 1.9349696386814117 | accuracy: 0.30368\n",
      "Epoch: 1 | loss: 1.8291597516107558 | accuracy: 0.34478\n",
      "Epoch: 2 | loss: 1.7830828502893448 | accuracy: 0.36162\n",
      "Epoch: 3 | loss: 1.7487359103488922 | accuracy: 0.37204\n",
      "Epoch: 4 | loss: 1.7215245046949386 | accuracy: 0.3821\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        total_loss += loss.data[0]\n",
    "        # Calculate no of correct classifications\n",
    "        _, predicted_class = outputs.max(1)\n",
    "        correct += predicted_class.data.eq(labels.data).sum()     \n",
    "        \n",
    "    print(\"Epoch: {0} | loss: {1} | accuracy: {2}\".format(epoch, total_loss/len(train_loader)\n",
    "                                                          , correct/float(len(train_loader.dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
